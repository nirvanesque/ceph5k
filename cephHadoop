#!/usr/bin/env ruby
# Copyright (c) 2015-17 Anirvan BASU, INRIA Rennes - Bretagne Atlantique
#
# Licensed under the CeCCIL-B license (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 

require 'cute'
require 'logger'
require 'cute/taktuk'
require 'net/sftp'
require 'erb'
require 'socket'
require 'trollop'
require 'json'
require "net/http"
require "uri"
require "fileutils"
require_relative "helpers/abstractions"


g5k = Cute::G5K::API.new()
user = g5k.g5k_user

# Get the script dir
scriptDir = File.expand_path(File.dirname(__FILE__))
# Make the temporary files directory (if not created already)
tempDir = scriptDir + "/.generated"
FileUtils.mkpath(tempDir)
FileUtils.mkpath(tempDir + "/config")

# Additionally create a directory for Hadoop config files
hadoopConfDir = tempDir + "/config/hadoop"
FileUtils.mkpath(hadoopConfDir)

currentConfigFile = tempDir + "/config/defaults.yml" # config file to be used.
# Read all options from CLI or config file
opts = readOptions(scriptDir, currentConfigFile, "cephDeploy")

# Prepare logFile for logging all actions
logger = logCreate(tempDir, "cephHadoop")

# Get job for Ceph clients (for deployed or Managed cluster)
jobCephClient = getJob(g5k, 0, opts[:'job-client'], opts[:site])

# If no Ceph clients found, abort Hadoop deployment
if jobCephClient.nil?
   logger.error "No Ceph client(s) found. First run cephClient or cephManaged script, then run this script."
   abort()
else
   logger.info "Ceph client job details recovered. Using them to create Hadoop cluster."
end # if jobCephClient.nil?


# Get and assign correctly the nodes in a Hadoop cluster
nodes = jobCephClient["assigned_nodes"]
master = ""
slaves = []
flagRestart = false
# nodeID (master, slave1, slave2, ... ) is stored in this file
nodeIDFile = opts[:'hadoop-cluster'] == "managed" ? "/mnt/" + opts[:'mnt-prod'] + "/hadoop/node-id" : "/mnt/" + opts[:'mnt-depl'] + "/hadoop/node-id" 

# If "restart" then reorder the "nodes" array
if opts[:'hadoop-cluster'] == "managed"
   unless opts[:'hadoop'] == "start"
      nodeID = ""
      newNodes = []
      nodes.each do |node|
         Cute::TakTuk.start([node], :user => "root") do |tak|
            result = tak.exec!("cat #{nodeIDFile}")
            nodeID = result[node][:output]

            # Get the FQDN of the "master"
            if nodeID.include? "master"
               newNodes[0] = node
            end

            # Get the FQDNs of the "slaves"
            if nodeID.include? "slave"
               slaveID = nodeID.slice(5..-1).to_i
               newNodes[slaveID] = node
            end

            tak.loop()
         end

      end # nodes.each do |node|
      # Assign new ordering to "nodes" ONLY if exact no. of nodes in master-slave config
      newNodesCount = newNodes.count{|newNode| !newNode.empty?}
      unless newNodes.empty? || nodes.count == newNodesCount
         nodes = newNodes
         flagRestart = true
      end

   end # unless opts[:'hadoop'] == "start"
end # if opts[:'hadoop-cluster'] == "managed"

master = nodes[0] # The first node in the list becomes the master
slaves = nodes - [master] # The rest of the nodes become slaves

# At this point job was created / fetched
logger.info "Deploying Hadoop cluster as follows:"
logger.info " All nodes: #{nodes}" 
logger.info "Master node on: #{master}"
logger.info "Slave node(s) on: #{slaves}"
logger.info "Hadoop binary download link : #{opts[:'hadoop-link']}"


# ssh key-sharing for password-less access from master node.
logger.info "ssh key-sharing for password-less access from master node ..."

# Prepare .ssh/config file locally
configFile = File.open("#{tempDir}/config/config", "w") do |file|
   nodes.each do |node|
      file.puts("Host #{node}")
      file.puts("   Hostname #{node}")
      file.puts("   User root")
      file.puts("   StrictHostKeyChecking no")
   end
end

# Get ssh_config file from master
Net::SFTP.start(master, 'root') do |sftp|
  sftp.download!("/etc/ssh/ssh_config", "#{tempDir}/config/ssh_config")
end

# In ssh_config file (local) add a line to avoid StrictHostKeyChecking
configFile = File.open("#{tempDir}/config/ssh_config", "a") do |file|
   file.logger.info("    StrictHostKeyChecking no") # append only once
end

# Copy ssh keys & config for Ceph on master node
ssh_key =  'id_rsa'
Cute::TakTuk.start([master], :user => "root") do |tak|
     tak.put("/home/#{user}/.ssh/#{ssh_key}", "/root/.ssh/#{ssh_key}") # copy the config file to master
     tak.put("/home/#{user}/.ssh/#{ssh_key}.pub", "/root/.ssh/#{ssh_key}.pub") # copy the config file to master
     tak.put("#{tempDir}/config/config", "/root/.ssh/config") # copy the config file to master
     tak.loop()
end

# Push ssh_config file & ssh public key to all nodes
Cute::TakTuk.start(nodes, :user => "root") do |tak|
     tak.put("#{tempDir}/config/ssh_config", "/etc/ssh/ssh_config")
     tak.put("/home/#{user}/.ssh/#{ssh_key}.pub", "/root/.ssh/#{ssh_key}.pub")
     tak.exec!("cat /root/.ssh/#{ssh_key}.pub >> /root/.ssh/authorized_keys")
     tak.exec!("chmod 600 /root/.ssh/config")
     tak.loop()
end

# ssh key-sharing completed.
logger.info "ssh key-sharing completed."


# Configuring Java path ...
logger.info "Configuring Java path ..."

javaHome = "/usr/lib/jvm/java-7-openjdk-amd64" # read from defaults.yml later
# Get /etc/profile file from master node
Net::SFTP.start(master, 'root') do |sftp|
  sftp.download!("/etc/profile", "#{hadoopConfDir}/profile")
end

# In profile file (local) add lines for Java path
configFile = File.open("#{hadoopConfDir}/profile", "a") do |file|
   file.puts("JAVA_HOME=#{javaHome}") # append following 3 lines
   file.puts("PATH=$PATH:$JAVA_HOME/bin")
   file.puts("export JAVA_HOME")
end

# Copy /etc/profile to master + slave nodes and "source" it
Cute::TakTuk.start(nodes, :user => "root") do |tak|
     tak.put("#{hadoopConfDir}/profile", "/etc/profile")
     tak.exec!(". /etc/profile")
end

# Java path configured.
logger.info "Java path configured."


# Some important hadoop directories
hadoopDir = "/opt/hadoop"
if opts[:'hadoop-cluster'] == "deployed"
   hadoopTmpDir = "/mnt/" + opts[:'mnt-depl'] + "/hadoop" # information to replace in .erb file
else
   hadoopTmpDir = "/mnt/" + opts[:'mnt-prod'] + "/hadoop" # information to replace in .erb file
end # if opts[:'hadoop-cluster'] == "managed"


# Stopping Hadoop
logger.info "Stopping any Hadoop daemons from master node ..."
Cute::TakTuk.start([master], :user => "root") do |tak|

     # Save namespace and tarzip file to managed Ceph mounted directory
     if opts[:'hadoop'] == "stop" # save namespace
        tak.exec!("#{hadoopDir}/bin/hdfs dfsadmin -safemode enter") # enter safemode
        tak.exec!("#{hadoopDir}/bin/hdfs dfsadmin -saveNamespace") # save Namespace
        tak.exec!("#{hadoopDir}/bin/hdfs dfsadmin -safemode leave") # leave safemode

        tak.exec!("rm -f /mnt/#{opts[:'mnt-prod']}/nameNode.tar.gz") 
        tak.exec!("tar czfP nameNode.tar.gz -C /mnt/#{opts[:'mnt-prod']}/ #{hadoopTmpDir}/dfs/nn && mv nameNode.tar.gz /mnt/#{opts[:'mnt-prod']}/")
     end # if opts[:'hadoop'] == "stop"

     # Stop previously running Hadoop daemons
     tak.exec!("#{hadoopDir}/sbin/stop-all.sh")

end

# Abort script if option "stop" was chosen.
abort("Hadoop cluster stopped. To recover data, please continue with option '--hadoop restart'") if opts[:'hadoop'] == "stop"

# Cleaning up Hadoop config files & temp directories
logger.info "Cleaning up Hadoop config files & temp directories ..."
Cute::TakTuk.start(nodes, :user => "root") do |tak|
     tak.exec!("rm -f #{hadoopDir}/etc/hadoop/core-site.xml #{hadoopDir}/etc/hadoop/hdfs-site.xml #{hadoopDir}/etc/hadoop/yarn-site.xml #{hadoopDir}/etc/hadoop/mapred-site.xml #{hadoopDir}/etc/hadoop/hadoop-env.sh")
     tak.exec!("mkdir -p #{hadoopTmpDir}/dfs #{hadoopTmpDir}/yarn #{hadoopTmpDir}/mapred")
     tak.exec!("chmod -R 777 #{hadoopTmpDir}")

     tak.loop()
end

# If Hadoop restart then restore Namenode directories
if opts[:'hadoop'] == "restart" 
   Cute::TakTuk.start([master], :user => "root") do |tak|
        tak.exec!("tar xzfP /mnt/#{opts[:'mnt-prod']}/nameNode.tar.gz -C #{hadoopTmpDir} --strip-components=3")  # Note the level of subdir (3) has to coincide with tar archive
   end
   logger.info "Hadoop Namenode directories reinstalled."
end # if opts[:'hadoop'] == "restart"


# Hadoop directory setup
logger.info "Getting binary tar file and installing Hadoop ..."

# Push hadoop tar file to all nodes & unpack
Cute::TakTuk.start(nodes, :user => "root") do |tak|
     tak.exec!("rm hadoop.tar.gz ; rm -rf #{hadoopDir} ; mkdir #{hadoopDir}")
     tak.exec!("wget -O hadoop.tar.gz #{opts[:'hadoop-link']}")
     tak.exec!("tar xzf hadoop.tar.gz -C #{hadoopDir} --strip-components=1")
     tak.loop()
end

# Hadoop directory setup completed
logger.info "Hadoop directory setup completed."


# Configuring Hadoop path ...
logger.info "Configuring Hadoop path ..."

# Get /root/.bashrc file from master node
Net::SFTP.start(master, 'root') do |sftp|
  sftp.download!("/root/.bashrc", "#{hadoopConfDir}/.bashrc")
end

# In .bashrc file (local) add lines for Hadoop path
configFile = File.open("#{hadoopConfDir}/.bashrc", "a") do |file|

   file.puts("# Hadoop parameters start here") 
   file.puts("export HADOOP_HOME=#{hadoopDir}")
   file.puts("export HADOOP_MAPRED_HOME=$HADOOP_HOME")
   file.puts("export HADOOP_COMMON_HOME=$HADOOP_HOME")
   file.puts("export HADOOP_HDFS_HOME=$HADOOP_HOME")
   file.puts("export YARN_HOME=$HADOOP_HOME")
   file.puts("export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop")
   file.puts("PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin")
   file.puts("# Hadoop parameters end here")

   file.puts("# HDFS parameters start here") 
   file.puts("export DFS_NAME_DIR=#{hadoopTmpDir}/hdfs/nn")
   file.puts("export FS_CHECKPOINT_DIR=#{hadoopTmpDir}/hdfs/snn")
   file.puts("export DFS_DATA_DIR=#{hadoopTmpDir}/hdfs/dn")
   file.puts("export HDFS_LOG_DIR=#{hadoopTmpDir}/hdfs/logs")
   file.puts("# HDFS parameters end here")

   file.puts("# YARN parameters start here") 
   file.puts("export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop")
   file.puts("export YARN_LOCAL_DIR=#{hadoopTmpDir}/yarn")
   file.puts("export YARN_LOG_DIR=#{hadoopTmpDir}/yarn/logs")
   file.puts("# YARN parameters end here")

   file.puts("# MapRed parameters start here") 
   file.puts("export MAPRED_LOG_DIR=#{hadoopTmpDir}/mapred/logs")
   file.puts("# MapRed parameters end here")

end

# Copy /root/.bashrc to master + slave nodes and "source" it
Cute::TakTuk.start(nodes, :user => "root") do |tak|
     tak.put("#{hadoopConfDir}/.bashrc", "/root/.bashrc")
     tak.exec!("bash")
     tak.loop()
end

# Hadoop path configured.
logger.info "Hadoop path configured."


# Listing master & slaves ...
logger.info "Listing master & slaves ..."

# Get /etc/hosts file from master node
Net::SFTP.start(master, 'root') do |sftp|
  sftp.download!("/etc/hosts", "#{hadoopConfDir}/hosts")
end

# In hosts file (local) add lines for master & slaves
configFile = File.open("#{hadoopConfDir}/hosts", "a") do |file|
   # append following lines

   file.puts("\n")
   masterIP = Socket.getaddrinfo(master, "http", nil, :STREAM)[0][2]
   file.puts("#{masterIP} master")
   slaves.each.with_index(1) do |slave, index| # loop over each slave
      slaveIP = Socket.getaddrinfo(slave, "http", nil, :STREAM)[0][2]
      file.puts("#{slaveIP} slave#{index}")
   end # slaves.each do |slave, index|

end

# Copy /etc/hosts to master + slave nodes
Cute::TakTuk.start(nodes, :user => "root") do |tak|
     tak.put("#{hadoopConfDir}/hosts", "/etc/hosts")
end

# Change master & slaves hostnames using hostnamectl
unless opts[:'hadoop'] == "stop"
   Cute::TakTuk.start([master], :user => "root") do |tak|
        # First change hostname of master to same name as in /etc/hosts
        tak.exec!("hostnamectl set-hostname master")
        tak.exec!("echo 'master' > #{nodeIDFile}")
        tak.loop()
   end

   # Then change hostname of each slave to same name as in /etc/hosts
   slaves.each.with_index(1) do |slave, index| # loop over each slave
      slaveIndex = "slave#{index}"
      Cute::TakTuk.start([slave], :user => "root") do |tak|
           tak.exec!("hostnamectl set-hostname #{slaveIndex}")
           tak.exec!("echo '#{slaveIndex}' > #{nodeIDFile}")
           tak.loop()
      end
   end # slaves.each do |slave, index|

end # unless opts[:'hadoop'] == "stop"

# In masters file add line for master
configFile = File.open("#{hadoopConfDir}/masters", "w") do |file|
   # append following line
   file.puts("master")
end

# In slaves file (local) add lines for slaves
configFile = File.open("#{hadoopConfDir}/slaves", "w") do |file|
   # append following lines
   slaves.each.with_index(1) do |slave, index| # loop over each slave
      file.puts("slave#{index}")
   end # slaves.each do |slave, index|
end

# Copy masters & slaves file to master
Cute::TakTuk.start([master], :user => "root") do |tak|
     tak.put("#{hadoopConfDir}/masters", "#{hadoopDir}/etc/hadoop/masters")
     tak.put("#{hadoopConfDir}/slaves", "#{hadoopDir}/etc/hadoop/slaves")
     tak.loop()
end

# Master & Slaves registered.
logger.info "Master & Slaves registered." 


# Prepare file core-site.xml
template = ERB.new File.new("./ceph5k/hadoop/core-site.xml.erb").read, nil, "%"
# Write result to config file core-site.xml
hadoopFileText = template.result(binding)
File.open("#{hadoopConfDir}/core-site.xml", "w") do |file|
   file.write(hadoopFileText)
end

# Prepare file hdfs-site.xml
numSlaves = slaves.count # information to replace in .erb file
dfsNamenodeNameDir = hadoopTmpDir + "/dfs/nn"
dfsSecNamenodeNameDir = hadoopTmpDir + "/dfs/snn"
dfsDatanodeDataDir = hadoopTmpDir + "/dfs/dn"
template = ERB.new File.new("./ceph5k/hadoop/hdfs-site.xml.erb").read, nil, "%"
# Write result to config file hdfs-site.xml
hadoopFileText = template.result(binding)
File.open("#{hadoopConfDir}/hdfs-site.xml", "w") do |file|
   file.write(hadoopFileText)
end

# Prepare file yarn-site.xml
yarnNodeMgrLocalDirs = hadoopTmpDir + "/yarn/local"
yarnNodeMgrLogDirs = hadoopTmpDir + "/yarn/log"
yarnNodeMgrRecovDirs = hadoopTmpDir + "/yarn/recovery"
template = ERB.new File.new("./ceph5k/hadoop/yarn-site.xml.erb").read, nil, "%"
# Write result to config file yarn-site.xml
hadoopFileText = template.result(binding)
File.open("#{hadoopConfDir}/yarn-site.xml", "w") do |file|
   file.write(hadoopFileText)
end

# Prepare file mapred-site.xml
mrClusterLocalDir = hadoopTmpDir + "/mapred/local"
mrJobtrackSystemDir = hadoopTmpDir + "/mapred/system"
mrJobtrackStagingRootDir = hadoopTmpDir + "/mapred/staging"
mrClusterTempDir = hadoopTmpDir + "/mapred/temp"
template = ERB.new File.new("./ceph5k/hadoop/mapred-site.xml.erb").read, nil, "%"
# Write result to config file mapred-site.xml
hadoopFileText = template.result(binding)
File.open("#{hadoopConfDir}/mapred-site.xml", "w") do |file|
   file.write(hadoopFileText)
end

# Prepare file hadoop-env.sh
template = ERB.new File.new("./ceph5k/hadoop/hadoop-env.sh.erb").read, nil, "%"
# Write result to config file hadoop-env.sh
hadoopFileText = template.result(binding)
File.open("#{hadoopConfDir}/hadoop-env.sh", "w") do |file|
   file.write(hadoopFileText)
end


# Then put 3 config files to all nodes
Cute::TakTuk.start(nodes, :user => "root") do |tak|
     tak.exec!("rm #{hadoopDir}/etc/hadoop/core-site.xml; rm #{hadoopDir}/etc/hadoop/hdfs-site.xml; rm #{hadoopDir}/etc/hadoop/yarn-site.xml; rm #{hadoopDir}/etc/hadoop/mapred-site.xml; rm #{hadoopDir}/etc/hadoop/hadoop-env.sh")
     tak.put("#{hadoopConfDir}/hadoop-env.sh", "#{hadoopDir}/etc/hadoop/hadoop-env.sh")
     tak.put("#{hadoopConfDir}/core-site.xml", "#{hadoopDir}/etc/hadoop/core-site.xml")
     tak.put("#{hadoopConfDir}/hdfs-site.xml", "#{hadoopDir}/etc/hadoop/hdfs-site.xml")
     tak.put("#{hadoopConfDir}/yarn-site.xml", "#{hadoopDir}/etc/hadoop/yarn-site.xml")
     tak.put("#{hadoopConfDir}/mapred-site.xml", "#{hadoopDir}/etc/hadoop/mapred-site.xml")
end

# Hadoop config files copied
logger.info "Hadoop config files copied."

# Additionally create a directory for saving details of Hadoop deployed
jobID = jobCephClient["uid"]
clientStateDir = tempDir + "/#{opts[:site]}/#{jobID}"
FileUtils.mkpath(clientStateDir)

# Copy Hadoop configuration files to Hadoop state directory
FileUtils.mkpath(clientStateDir + "/hadoop")
FileUtils.cp("#{hadoopConfDir}/core-site.xml", "#{clientStateDir}/hadoop/core-site.xml")
FileUtils.cp("#{hadoopConfDir}/hdfs-site.xml", "#{clientStateDir}/hadoop/hdfs-site.xml")
FileUtils.cp("#{hadoopConfDir}/yarn-site.xml", "#{clientStateDir}/hadoop/yarn-site.xml")
FileUtils.cp("#{hadoopConfDir}/yarn-site.xml", "#{clientStateDir}/hadoop/mapred-site.xml")
FileUtils.cp("#{hadoopConfDir}/yarn-site.xml", "#{clientStateDir}/hadoop/hadoop-env.sh")
FileUtils.cp("#{hadoopConfDir}/masters", "#{clientStateDir}/hadoop/masters")
FileUtils.cp("#{hadoopConfDir}/slaves", "#{clientStateDir}/hadoop/slaves")



# Starting Hadoop daemons 
Cute::TakTuk.start([master], :user => "root") do |tak|
     if opts[:'hadoop'] == "restart" && flagRestart == true
        # Do "restart" of Hadoop namenode ONLY if both conditions are true
        logger.info "Preparing & restarting Hadoop daemons from master node ..."
        tak.exec!("#{hadoopDir}/bin/hdfs namenode --config $HADOOP_CONF_DIR") # start namenode
     else
        # Format namenode & start
        logger.info "Preparing & starting Hadoop daemons from master node ..."
        tak.exec!("#{hadoopDir}/bin/hdfs namenode -format -force") 
     end # if opts[:'hadoop'] == "restart"

     tak.exec!("#{hadoopDir}/sbin/start-all.sh") # start all Hadoop daemons

     tak.exec!("#{hadoopDir}/bin/hdfs dfsadmin -safemode leave") # leave safemode

#     tak.exec!("#{hadoopDir}/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver") # start MapReduce jobhistory server
end

# Hadoop started
logger.info "Hadoop cluster started with Master node: #{master}"
# Next only if Hadoop restart
logger.info "All previous data retained." if opts[:'hadoop'] == "restart"
logger.info "Login to master node to submit jobs!"

