#!/usr/bin/env ruby
# Copyright (c) 2015-17 Anirvan BASU, INRIA Rennes - Bretagne Atlantique
#
# Licensed under the CeCCIL-B license (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 

require 'cute'
require 'logger'
require 'cute/taktuk'
require 'net/sftp'
require 'erb'
require 'socket'
require 'trollop'
require 'json'
require "net/http"
require "uri"
require "fileutils"
require_relative "helpers/abstractions"


g5k = Cute::G5K::API.new()
user = g5k.g5k_user

# Get the script dir
scriptDir = File.expand_path(File.dirname(__FILE__))
# Make the temporary files directory (if not created already)
tempDir = scriptDir + "/.generated"
FileUtils.mkpath(tempDir)
FileUtils.mkpath(tempDir + "/config")

currentConfigFile = tempDir + "/config/defaults.yml" # config file to be used.
opts = readOptions(scriptDir, currentConfigFile, "cephClient")

# Adapt some CLI arguments into global variables. Later change to class attributes.
opts[:'pool-name'] = "#{user}_" + opts[:'pool-name'] # Name of pool to create on clusters.
opts[:'rbd-name'] = "#{user}_" + opts[:'rbd-name'] # Name of pool to create on clusters.


# Initialise some global variables"
jobCephCluster = nil # Job for deployed Ceph cluster
monitor = "" # Monitor for deployed Ceph cluster
# Get all jobs submitted in a cluster
jobs = g5k.get_my_jobs(opts[:site], state = "running") 

# get the Ceph deploy job with name "cephDeploy" or opts[:'job-name']
jobs.each do |job|
   if job["name"] == opts[:'job-name'] # if job exists already, get nodes
      jobCephCluster = job
      monitor = jobCephCluster["assigned_nodes"][0]
   end # if job["name"] == opts[:'job-name']
end # jobs.each do |job|

# Initialise some global variables
jobCephClient = nil # Ceph client job
clients = [] # Array of client nodes

# If client-list specified in CLI argument, get list of clients & fill variable
unless opts[:'file'].empty?
   clients = File.open(opts[:'file'], 'r'){ |file| 
      file.readlines.collect{ |line| line.chomp }
   } # File.open

   # Recover job details of client-list
   jobs = g5k.get_my_jobs(opts[:'client-site'], state = "running") 

   jobs.each do |job|
      if job["assigned_nodes"] == clients # get job where nodes are same as client-list
         jobCephClient = job
         puts "Ceph client job recovered with nodes: #{clients}"
       end # if job["assigned_nodes"] == clients
   end # jobs.each do |job|

else # when no client-list is specified. Do deployment or start from scratch.

   unless [nil, 0].include?(opts[:jobid])
      # If jobID is specified, get the specific job
      jobCephClient = g5k.get_job(opts[:'client-site'], opts[:jobid])
   else
      # Get all my jobs submitted in a site
      jobs = []
      ["waiting","running"].each do |state|
         jobs += g5k.get_my_jobs(opts[:'client-site'], state)

         # get the job with name "cephClient" or opts[:'job-client']
         jobs.each do |job|
            if job["name"] == opts[:'job-client'] # if job exists already, get nodes
               jobCephClient = job
            end # if job["name"] == opts[:'job-client']

         end # jobs.each do |job|

      end # ["waiting","running"].each do |state|

   end # if opts[:jobid]

   # If job state is "waiting" then wait for resources to be assigned
   unless jobCephClient.nil?

      if jobCephClient["state"] == "waiting"
         begin
            job = g5k.wait_for_job(jobCephClient, :wait_time => 60)
         rescue Cute::G5K::EventTimeout
            puts "Waited too long in site #{opts[:'client-site']}, releasing job #{opts[:'job-client']}"
            g5k.release(job)
         end
      end # if jobCephClient["state"] == "waiting"

   else
      # Finally, if job does not yet exist reserve nodes
      jobCephClient = g5k.reserve(:name => opts[:'job-client'], :nodes => opts[:'num-client'], :site => opts[:'client-site'], :cluster => opts[:'client-cluster'], :walltime => opts[:walltime], :type => :deploy) 

   end # unless jobCephClient.nil?

   # Assign roles to each node
   clients = jobCephClient["assigned_nodes"]

   # if not yet deployed, then deploy it
   if jobCephClient["deploy"].nil?
      puts "Deploying #{opts[:'env-client']} on client node(s): #{clients}"
      depCephClient = g5k.deploy(jobCephClient, :nodes => clients, :env => opts[:'env-client']) 
      g5k.wait_for_deploy(jobCephClient)
   end # if jobCephClient["deploy"].nil?



end # unless opts[:'file'].empty?

# At this point Ceph client job was created / fetched
puts "Deploying Ceph clients on nodes: #{clients}" 


# Additionally create a directory for saving details of clients deployed
jobID = jobCephClient["uid"]
clientStateDir = tempDir + "/#{opts[:'client-site']}/#{jobID}"
FileUtils.mkpath(clientStateDir)

# Prepare clients-list-file locally
clientsFile = File.open("#{clientStateDir}/clients-list", "w") do |file|
   clients.each do |client|
      file.puts("#{client}")
   end
end

# Abort script if only Linux deployment flag was set
abort("Linux #{opts[:'env-client']} deployed on clients: #{clients}. \n Clients list file in: #{clientStateDir}/clients-list. \n Rerun script with option -f <nodes list file> to configure Ceph clients.") if opts[:'only-deploy']


# Abort script if no deployed Ceph cluster
abort("No deployed Ceph cluster found. First deploy Ceph cluster, then run script.") if jobCephCluster.nil?

# Remind where is the deployed Ceph monitor
puts "Deployed Ceph cluster details:"
puts "   monitor on: #{monitor}"


#1 Preflight Checklist
puts "Doing pre-flight checklist..."

# Add Ceph & Extras to each Ceph node ('firefly' is the most complete)
argDebian = opts[:'env-client'].slice(0,6)

# Specify explicit dependencies for Ceph packages 
# See Bug #832714: Ceph from Jessie-backports
aptgetPurgeCmd = ""
aptgetInstallCmd = ""
if argDebian.include? "jessie"
   aptgetPurgeCmd = "apt-get -y autoremove ceph ceph ceph-common libcephfs1 librados2 librbd1 python-ceph && apt-get -y purge ceph ceph-common libcephfs1 librados2 librbd1 python-ceph"
   aptgetInstallCmd = "apt-get -y --force-yes install ceph=0.80.10-2~bpo8+1 chrony ceph-common=0.80.10-2~bpo8+1 python-ceph=0.80.10-2~bpo8+1 librbd1=0.80.10-2~bpo8+1 libcephfs1=0.80.10-2~bpo8+1 librados2=0.80.10-2~bpo8+1"
else 
   aptgetPurgeCmd = "apt-get -y autoremove ceph ceph-common && apt-get -y purge ceph ceph-common"
   aptgetInstallCmd = "apt-get -y update && apt-get -y --force-yes install ceph"
end # if argDebian.include? "jessie"

Cute::TakTuk.start(clients, :user => "root") do |tak|
     tak.exec!("echo deb http://apt.grid5000.fr/ceph5k/#{argDebian}/#{opts[:release]} / | tee /etc/apt/sources.list.d/ceph.list")
     tak.exec!(aptgetPurgeCmd) # Purge previous ceph packages & dependencies
     tak.exec!(aptgetInstallCmd) # Install ceph packages & dependencies
     tak.loop()
end


# Prepare config file locally with list of nodes for clients
configFile = File.open("#{tempDir}/config/config", "w") do |file|
   clients.each do |client|
      file.puts("Host #{client}")
      file.puts("   Hostname #{client}")
      file.puts("   User root")
      file.puts("   StrictHostKeyChecking no")
   end # clients.each do |client|

end

# Get ssh_config file from master/monitor
Net::SFTP.start(monitor, 'root') do |sftp|
  sftp.download!("/etc/ssh/ssh_config", "#{tempDir}/config/ssh_config")
end

# Copy first updated config for Ceph clients on monitor/master node
ssh_key =  'id_rsa'
Cute::TakTuk.start([monitor], :user => "root") do |tak|
     result = tak.put("#{tempDir}/config/config", "/root/.ssh/config") # copy the config file to master/monitor
     tak.loop()
end

# Push ssh_config file & ssh public key to all nodes
Cute::TakTuk.start(clients, :user => "root") do |tak|
     tak.put("#{tempDir}/config/ssh_config", "/etc/ssh/ssh_config")
     tak.put("/home/#{user}/.ssh/#{ssh_key}.pub", "/root/.ssh/#{ssh_key}.pub")
     tak.exec!("cat /root/.ssh/#{ssh_key}.pub >> /root/.ssh/authorized_keys")
     tak.loop()
end

# Preflight checklist completed.
puts "Pre-flight checklist completed." + "\n"


# Install & administer clients to Ceph deployed cluster.
puts "Adding following clients to deployed Ceph cluster:"
clients.each do |client|
     clientShort = client.split(".").first
     Cute::TakTuk.start([monitor], :user => "root") do |tak|
          tak.exec!("ceph-deploy install --release #{opts[:release]} #{clientShort}")
          result = tak.exec!("ceph-deploy --overwrite-conf admin #{clientShort}")
          puts "Added client: #{client}" if result[monitor][:status] == 0
          tak.loop()
     end
end # clients.each do


# Create Ceph pools on deployed cluster.
puts "Creating Ceph pools on deployed cluster ..."
# Create Ceph pools & RBDs
Cute::TakTuk.start(clients, :user => "root") do |tak|
     tak.exec!("modprobe rbd")
     tak.exec!("rados mkpool #{opts[:'pool-name']}")
     tak.exec!("rbd create --image-format 2 #{opts[:'rbd-name']} --pool #{opts[:'pool-name']} --size #{opts[:'rbd-size']}")
     tak.loop()
end

# Created Pools & RBDs for Ceph deployed cluster.
puts "Created Ceph pool on deployed cluster as follows :" + "\n"
puts "Pool name: #{opts[:'pool-name']} , RBD Name: #{opts[:'rbd-name']} , RBD Size: #{opts[:'rbd-size']} " + "\n"


# Map RBDs and create File Systems.
puts "Mapping RBD in deployed Ceph clusters ..."
Cute::TakTuk.start(clients, :user => "root") do |tak|
     # Map RBD & create FS on deployed cluster
     tak.exec!("rbd map #{opts[:'rbd-name']} --pool #{opts[:'pool-name']}")
     tak.exec!("mkfs.#{opts[:'file-system']} -m0 /dev/rbd/#{opts[:'pool-name']}/#{opts[:'rbd-name']}")
     tak.loop()
end
# Mapped RBDs & created FS for clients on Ceph deployed cluster.
puts "Mapped RBDs #{opts[:'rbd-name']} for clients on deployed Ceph." + "\n"


# Mount RBDs on Ceph client(s).
puts "Mounting RBDs in deployed Ceph cluster on client(s) ..."
clients.each do |client|
   Cute::TakTuk.start([client], :user => "root") do |tak|

        # mount RBD from deployed cluster
        tak.exec!("umount /dev/rbd/#{opts[:'pool-name']}/#{opts[:'rbd-name']} /mnt/#{opts[:'mnt-depl']}")
        tak.exec!("rmdir /mnt/#{opts[:'mnt-depl']}")
        tak.exec!("mkdir /mnt/#{opts[:'mnt-depl']}")
        result = tak.exec!("mount /dev/rbd/#{opts[:'pool-name']}/#{opts[:'rbd-name']} /mnt/#{opts[:'mnt-depl']}")
        puts "Mounted RBD as File System on client: #{client}" if result[client][:status] == 0

        tak.loop()
   end
end # clients.each do

