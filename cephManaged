#!/usr/bin/env ruby
# Copyright (c) 2015-17 Anirvan BASU, INRIA Rennes - Bretagne Atlantique
#
# Licensed under the CeCCIL-B license (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 

require 'cute'
require 'logger'
require 'cute/taktuk'
require 'net/sftp'
require 'erb'
require 'socket'
require 'trollop'
require 'json'
require "net/http"
require "uri"
require "fileutils"


g5k = Cute::G5K::API.new()
user = g5k.g5k_user

# Get the script dir
scriptDir = File.expand_path(File.dirname(__FILE__))
# Make the temporary files directory (if not created already)
tempDir = scriptDir + "/.generated"
FileUtils.mkpath(tempDir)
FileUtils.mkpath(tempDir + "/config")

# Additionally create a directory for managed Ceph config files
prodDir = tempDir + "/config/prod"
FileUtils.mkpath(prodDir)

currentConfigFile = ""
if (["--def-conf", "-d"].include?(ARGV[0])  && !ARGV[1].empty? )
   currentConfigFile = ARGV[1] # assign config file location to variable configFile
   ARGV.delete_at(0)    # clean up ARGV array
   ARGV.delete_at(0)
else 
   currentConfigFile = tempDir + "/config/defaults.yml" # config file to be used.
   unless File.exist?(currentConfigFile)
     configFile = scriptDir + "/config/defaults.yml.example" # example config file
     FileUtils.cp(configFile, currentConfigFile)
   end # unless File.exist?
end    # if (["--def-conf", "-d"])

# Populate the hash with default parameters from YAML file.
defaults = begin
  YAML.load(File.open(currentConfigFile))
rescue ArgumentError => e
  puts "Could not parse YAML: #{e.message}"
end


# banner for script
opts = Trollop::options do
  version "ceph5k 0.0.8 (c) 2015-16 Anirvan BASU, INRIA RBA"
  banner <<-EOS
cephManaged is a script for creating RBD and FS on the Managed Ceph clusters in Grid'5000

Usage:
       cephManaged [options]
where [options] are:
EOS

  opt :ignore, "Ignore incorrect values"
  opt :jobid, "Oarsub ID of the client job", :default => 0
  opt :site, "Grid 5000 site for deploying Ceph cluster", :type => String, :default => defaults["site"]
  opt :cluster, "Grid 5000 cluster in specified site", :type => String, :default => defaults["cluster"]
  opt :walltime, "Wall time for Ceph cluster deployed", :type => String, :default => defaults["walltime"]

  opt :release, "Ceph Release name", :type => String, :default => defaults["release"]
  opt :'pool-name', "Pool name on Ceph cluster (userid_ added)", :type => String, :default => defaults["client-pool-name"]
  opt :'pool-size', "Pool size on Ceph cluster", :default => defaults["client-pool-size"]
  opt :'rbd-name', "RBD name for Ceph pool (userid_ added)", :type => String, :default => defaults["client-rbd-name"]
  opt :'rbd-size', "RBD size on Ceph pool", :default => defaults["client-rbd-size"]
  opt :'file', "File with clients list, same option as in kadeploy3", :type => String, :default => ""
  opt :'file-system', "File System to be formatted on created RBDs", :type => String, :default => defaults["file-system"]
  opt :'mnt-prod', "Mount point for RBD on managed cluster", :type => String, :default => defaults["mnt-prod"]

  opt :'job-client', "Grid'5000 job name for Ceph clients", :type => String, :default => defaults["job-client"]
  opt :'env-client', "G5K environment for client", :type => String, :default => defaults["env-client"]
  opt :'managed-cluster', "site for managed Ceph cluster: 'rennes' or 'nantes'", :type => String, :default => defaults["managed-cluster"]
  opt :'multi-client', "Multiple clients to access Ceph Managed cluster", :default => defaults["multi-client"]
  opt :'num-client', "Nodes in Ceph Client cluster", :default => defaults["num-client"]
  opt :'no-deployed', "Not using any deployed Ceph cluster", :default => defaults["no-deployed"]
  opt :'rbd-list-file', "YAML file with RBD list. No. of RBDs must be same as no. of clients", :type => String, :default => ""

end

# Move CLI arguments into variables. Later change to class attributes.
argJobID = opts[:jobid] # Oarsub ID of the client job. 
argSite = opts[:site] # site name. 
argG5KCluster = opts[:cluster] # G5K cluster name if specified. 
argWallTime = opts[:walltime] # walltime for the client reservation.

argRelease = opts[:release] # Ceph release name. 
argPoolName = "#{user}_" + opts[:'pool-name'] # Name of pool to create on clusters.
argPoolSize = opts[:'pool-size'] # Size of pool to create on clusters.
argRBDName = "#{user}_" + opts[:'rbd-name'] # Name of pool to create on clusters.
argRBDSize = opts[:'rbd-size'] # Size of pool to create on clusters.
argFileSystem = opts[:'file-system'] # File System to be formatted on created RBDs.
argMntProd = opts[:'mnt-prod'] # Mount point for RBD on production cluster.

argClientList = opts[:'file'] # File with clients list.
argOnlyDeploy = opts[:'only-deploy'] # Only deploy linux but don't configure Ceph client.
argEnvClient = opts[:'env-client'] # Grid'5000 environment to deploy Ceph clients. 
argJobClient = opts[:'job-client'] # Grid'5000 job name for Ceph clients. 
argManagedCluster = opts[:'managed-cluster'] # managed Ceph cluster: 'rennes' or 'nantes'. 
argMultiClient = opts[:'multi-client'] # Multiple clients to access Ceph Managed cluster.
argNumClient = opts[:'num-client'] # Nodes in Ceph Client cluster.
argNoDeployed = opts[:'no-deployed'] # Not using any deployed Ceph cluster.
argRBDListFile = opts[:'rbd-list-file'] # YAML file with RBD list. No. of RBDs must be same as no. of clients.


# Next get job for Ceph clients
jobCephClient = nil # Ceph client job
clients = [] # Array of client nodes

# If client-list specified in CLI argument, get list of clients & fill variable
unless argClientList.empty?
   clients = File.open(argClientList, 'r'){ |file| 
      file.readlines.collect{ |line| line.chomp }
   }

   # Recover job details of client-list
   jobs = g5k.get_my_jobs(argSite, state = "running") 

   jobs.each do |job|
      if job["assigned_nodes"] == clients # get job where nodes are same as client-list
         jobCephClient = job
         puts "Ceph client job recovered with nodes: #{clients}"
       end # if job["assigned_nodes"] == clients
   end # jobs.each do |job|

else # when no client-list is specified. Do deployment or start from scratch.

   unless [nil, 0].include?(argJobID)
      # If jobID is specified, get the specific job
      jobCephClient = g5k.get_job(argSite, argJobID)
   else
      # Get all my jobs submitted in a site
      jobs = []
      ["waiting","running"].each do |state|
         jobs += g5k.get_my_jobs(argSite, state)

         # get the job with name "cephClient" or argJobName
         jobs.each do |job|
            if job["name"] == argJobClient # if job exists already, get nodes
               jobCephClient = job
            end # if job["name"] == argJobClient

         end # jobs.each do |job|

      end # ["waiting","running"].each do |state|

   end # if argJobID

   # If job state is "waiting" then wait for resources to be assigned
   unless jobCephClient.nil?

      if jobCephClient["state"] == "waiting"
         begin
            job = g5k.wait_for_job(jobCephClient, :wait_time => 60)
         rescue Cute::G5K::EventTimeout
            puts "Waited too long in site #{argSite}, releasing job #{argJobClient}"
            g5k.release(job)
         end
      end # if jobCephClient["state"] == "waiting"

   else
      # Finally, if job does not yet exist reserve nodes
      jobCephClient = g5k.reserve(:name => argJobClient, :nodes => argNumClient, :site => argSite, :cluster => argG5KCluster, :walltime => argWallTime, :type => :deploy) 

   end # unless jobCephClient.nil?

   # Assign roles to each node
   clients = jobCephClient["assigned_nodes"]

end # unless argClientList.empty?


# Additionally create a directory for saving details of clients deployed
jobID = jobCephClient["uid"]
clientStateDir = tempDir + "/#{argSite}/#{jobID}"
FileUtils.mkpath(clientStateDir)

# Prepare clients-list-file locally
clientsFile = File.open("#{clientStateDir}/clients-list", "w") do |file|
   clients.each do |client|
      file.puts("#{client}")
   end
end

# Abort script if only Linux deployment flag was set
abort("Linux #{argEnvClient} deployed on clients: #{clients}. \n Clients list file in: #{clientStateDir}/clients-list. \n Rerun script with option -f <nodes list file> to configure Ceph clients.") if argOnlyDeploy


# Get the client(s) for the managed Ceph cluster
# For a single client, this is the 'first' node of the job
clients = argMultiClient ? jobCephClient["assigned_nodes"] : [jobCephClient["assigned_nodes"][0]]

# if not yet deployed, then deploy it
if jobCephClient["deploy"].nil?
   puts "Deploying #{argEnvClient} on client node(s): #{clients}"
   depCephClient = g5k.deploy(jobCephClient, :nodes => clients, :env => argEnvClient) 
   g5k.wait_for_deploy(jobCephClient)
end # if jobCephClient["deploy"].nil?

# Remind where is the Ceph client
puts "Managed Ceph client(s) on: #{clients}"


#1 Preflight Checklist
puts "Doing pre-flight checklist..."

# Add Ceph & Extras to each Ceph node ('firefly' is the most complete)
argDebian = argEnvClient.slice(0,6)

# Specify explicit dependencies for Ceph packages 
# See Bug #832714: Ceph from Jessie-backports
aptgetPurgeCmd = ""
aptgetInstallCmd = ""
if argDebian.include? "jessie"
   aptgetPurgeCmd = "apt-get -y autoremove ceph ceph ceph-common libcephfs1 librados2 librbd1 python-ceph && apt-get -y purge ceph ceph-common libcephfs1 librados2 librbd1 python-ceph"
   aptgetInstallCmd = "apt-get -y --force-yes install ceph=0.80.10-2~bpo8+1 chrony ceph-common=0.80.10-2~bpo8+1 python-ceph=0.80.10-2~bpo8+1 librbd1=0.80.10-2~bpo8+1 libcephfs1=0.80.10-2~bpo8+1 librados2=0.80.10-2~bpo8+1"
else 
   aptgetPurgeCmd = "apt-get -y autoremove ceph ceph-common && apt-get -y purge ceph ceph-common"
   aptgetInstallCmd = "apt-get -y update && apt-get -y --force-yes install ceph"
end # if argDebian.include? "jessie"

Cute::TakTuk.start(clients, :user => "root") do |tak|
#     result = tak.exec!("apt-get autoremove -y ceph && apt-get autoremove -y ceph-deploy")
#     tak.exec!("echo deb #{ceph_extras}  | sudo tee /etc/apt/sources.list.d/ceph-extras.list")
     tak.exec!("echo deb http://apt.grid5000.fr/ceph5k/#{argDebian}/#{argRelease} / | tee /etc/apt/sources.list.d/ceph.list")
     tak.exec!(aptgetPurgeCmd) # Purge previous ceph packages & dependencies
     tak.exec!(aptgetInstallCmd) # Install ceph packages & dependencies
     tak.loop()
end

# Preflight checklist completed.
puts "Pre-flight checklist completed."


# Prepare ceph.conf file for managed Ceph cluster
FileUtils.mkpath(prodDir + "/#{argManagedCluster}")
configFile = File.open(prodDir + "/#{argManagedCluster}/ceph.conf", "w") do |file|
   file.puts("[global]")
   file.puts("  mon initial members = ceph0")
   if argManagedCluster == "rennes"
      file.puts("  mon host = 172.16.111.30")
   else # nantes cluster
      file.puts("  mon host = 172.16.207.10")
   end # if managedCluster
end

# Copy ceph.conf file to client state directory
FileUtils.mkpath(clientStateDir + "/prod/#{argManagedCluster}")
FileUtils.cp("#{prodDir}/#{argManagedCluster}/ceph.conf", "#{clientStateDir}/prod/#{argManagedCluster}/ceph.conf")
FileUtils.cp("/tmp/#{argManagedCluster}/ceph.client.#{user}.keyring", "#{clientStateDir}/prod/#{argManagedCluster}/ceph.client.#{user}.keyring")

# Then put ceph.conf file to the client
Cute::TakTuk.start(clients, :user => "root") do |tak|
     tak.exec!("mkdir -p /etc/ceph/#{argManagedCluster} && touch /etc/ceph/#{argManagedCluster}/ceph.client.#{user}.keyring")
     tak.exec!("mkdir -p prod/#{argManagedCluster} && touch prod/#{argManagedCluster}/ceph.conf")
     tak.put("#{prodDir}/#{argManagedCluster}/ceph.conf", "/root/prod/#{argManagedCluster}/ceph.conf")
     tak.put("/tmp/#{argManagedCluster}/ceph.client.#{user}.keyring", "/etc/ceph/#{argManagedCluster}/ceph.client.#{user}.keyring")
     tak.loop()
end

# Created & pushed config file for Managed Ceph cluster.
puts "Created & pushed config file for managed Ceph cluster to client(s)."


# Creating Ceph pools on managed cluster.
puts "Configuring Ceph pool & RBD on managed cluster ..."
poolsList = []
userPool = ""
userPoolMatch = ""
userRBD = ""
prodCluster = false
abortFlag = false
# Check Ceph pools & RBD on Managed cluster, using first client.
client = clients[0]
Cute::TakTuk.start([client], :user => "root") do |tak|
     tak.exec!("modprobe rbd")
     # Create RBD on managed cluster
     result = tak.exec!("rados -c /root/prod/#{argManagedCluster}/ceph.conf --id #{user} -k /etc/ceph/#{argManagedCluster}/ceph.client.#{user}.keyring lspools")

     poolsList = result[client][:output].split("\n") # Get list of pools in an array

     poolsList.each do |pool|  # logic: it will take the alphabetic-last pool from user
        userPool = pool if pool == argPoolName
     end # poolsList.each do

     if userPool.empty?   # There is no pool created on managed Ceph
        puts "No Ceph pool found with name: #{argPoolName}"
        puts "Specify correct pool name in config file or using option --client-pool-name"
        puts "Or verify / create the Ceph pool from the Ceph managed cluster frontend"
        puts "Use this link to create pool: https://api.grid5000.fr/sid/storage/ceph/ui/"
        puts "Then rerun this script."
        abortFlag = true
     end # userPool.empty?
     tak.loop()
end

# Abort script if no pool in managed Ceph
abort("Script exited - no pool for user in Managed Ceph clusters") if abortFlag

# At this point pool identified on Managed Ceph, now identify/create RBDs

# Get list of RBD images in specified Ceph "pool", as an array
poolRBDList = [] 
firstClient = clients[0]
Cute::TakTuk.start([firstClient], :user => "root") do |tak|
     result = tak.exec!("rbd -c /root/prod/#{argManagedCluster}/ceph.conf --id #{user} -k /etc/ceph/#{argManagedCluster}/ceph.client.#{user}.keyring --pool #{userPool} ls")
     poolRBDList = result[firstClient][:output].split("\n") unless result[firstClient][:output].nil?
     tak.loop()
end

abort("Script exited - number mismatch between list of RBDs in pool #{userPool} and number of clients: #{argNumClient}") unless poolRBDList.count == 0 || poolRBDList.count == argNumClient

# Get list of client RBD images if specified in file
clientRBDList = nil
unless argRBDListFile.nil?
   clientRBDList = begin
     YAML.load(File.open(argRBDListFile))
   rescue ArgumentError => e
     puts "Could not parse YAML: #{e.message}"
   end

puts "List of client RBDs: #{clientRBDList}"

   abort("Script exited - mismatch between supplied list of RBDs and RBDs found in pool #{userPool}") unless clientRBDList & poolRBDList == clientRBDList || poolRBDList.count == 0
   abort("Script exited - Count mismatch between supplied list of RBDs and number of clients: #{argNumClient}") unless clientRBDList.count == argNumClient


   # Verify if number of client RBDs specified 
end # unless argRBDListFile.nil?

Cute::TakTuk.start(clients, :user => "root") do |tak|

     case
     when rbdList.count == argNumClient
        puts "Identified following RBD images: #{rbdList}"
     when rbdList.count > argNumClient
        puts "Identified following RBD images: #{rbdList}. Will use first subset for clients."
     else # rbdList.count <= argNumClient
        puts "RBDs insufficient for #{argNumClient} clients. Creating new ... "
     end

     if userRBD.empty? # There was no rbd created for the user. So create it.
        results = tak.exec!("rbd -c /root/prod/#{argManagedCluster}/ceph.conf --id #{user} --pool #{userPool} create #{argRBDName} --size #{argRBDSize} -k /etc/ceph/#{argManagedCluster}/ceph.client.#{user}.keyring")

        results.each do |client, result|
           puts "RBD #{argRBDName} created on Ceph managed cluster for client #{client}." if result[:status] == 0
        end # results.each do |client, result|

     end # if userRBD.empty?
     tak.loop()
end

# Configured Pool and created RBD for Ceph cluster.
unless userPool.empty?
     puts "Configured/Created Pool and RBD as follows :"
     puts "On managed cluster:\n"
     puts "Pool name: #{userPool} , RBD Name: #{argRBDName} , RBD Size: #{argRBDSize} MB"
end # unless userPool.empty?


if argMultiClient
   clients.each.with_index(0) do |client, index| # loop over each client to create RBD

      Cute::TakTuk.start(clients, :user => "root") do |tak|
           # Check if RBD is already created, may contain data
           resultRBD = tak.exec!("rbd -c /root/prod/#{argManagedCluster}/ceph.conf --id #{user} --pool #{userPool} -k /etc/ceph/#{argManagedCluster}/ceph.client.#{user}.keyring ls")

           unless resultRBD[client][:output].nil? # means no rbd in userPool
              if resultRBD[client][:output].include? "#{argRBDName}" 
                 userRBD = argRBDName # There is an rbd with name argRBDName already
                 puts "RBD #{argRBDName} already exists in Ceph managed cluster. Using it."
              end # if resultPool[client][:output].include?
           end # unless resultPool[client][:output].nil?

      end

   end # slaves.each do |client, index|

else

# Map RBD and create File Systems.
puts "Mapping RBD in managed Ceph clusters ..."
Cute::TakTuk.start(clients, :user => "root") do |tak|
     unless argNoDeployed
        # Map RBD & create FS on deployed Ceph cluster
        results = tak.exec!("rbd map #{argRBDName} --pool #{argPoolName}")
        tak.exec!("mkfs.#{argFileSystem} /dev/rbd/#{argPoolName}/#{argRBDName}")

        results.each do |client, result|
           puts "Mapped RBD #{myRBDName} on deployed Ceph to client #{client}." if result[:status] == 0
        end # results.each do |client, result|

     end # if argNoDeployed

     myRBDName = ""
     # Map RBD & create FS on managed cluster
     results = tak.exec!("rbd -c /root/prod/#{argManagedCluster}/ceph.conf --id #{user} --pool #{userPool} map #{argRBDName} -k /etc/ceph/#{argManagedCluster}/ceph.client.#{user}.keyring")
     if userRBD.empty? # Do it only the first time when the RBD is created.
        tak.exec!("mkfs.#{argFileSystem} /dev/rbd/#{userPool}/#{argRBDName}")
        myRBDName = argRBDName
     else              # This case is when RBD is already created earlier.
        myRBDName = userRBD
     end # if userRBD.empty?

     tak.loop()
     results.each do |client, result|
        puts "Mapped RBD #{myRBDName} on managed Ceph to client #{client}." if result[:status] == 0
     end # results.each do |client, result|
end


# Mount RBDs as File Systems.
puts "Mounting RBD as File Systems in managed Ceph cluster ..."
Cute::TakTuk.start(clients, :user => "root") do |tak|

     result = nil
     # mount RBD from managed cluster
     if userRBD.empty? # Do it only the first time when the RBD is created.
        tak.exec!("umount /dev/rbd/#{userPool}/#{argRBDName} /mnt/#{argMntProd}")
        tak.exec!("rmdir /mnt/#{argMntProd}")
        tak.exec!("mkdir /mnt/#{argMntProd}")
        results = tak.exec!("mount /dev/rbd/#{userPool}/#{argRBDName} /mnt/#{argMntProd}")
     else              # This case is when RBD is already created earlier.
        tak.exec!("umount /dev/rbd/#{userPool}/#{userRBD} /mnt/#{argMntProd}")
        tak.exec!("rmdir /mnt/#{argMntProd}")
        tak.exec!("mkdir /mnt/#{argMntProd}")
        results = tak.exec!("mount /dev/rbd/#{userPool}/#{userRBD} /mnt/#{argMntProd}")
     end # if userRBD.empty?
     results.each do |client, result|
        puts "Mounted RBD on managed Ceph to client #{client}." if result[:status] == 0
     end # results.each do |client, result|

     tak.loop()
end
end # if argMultiClient
