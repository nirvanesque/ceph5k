#!/usr/bin/env ruby
# Copyright (c) 2015-17 Anirvan BASU, INRIA Rennes - Bretagne Atlantique
#
# Licensed under the CeCCIL-B license (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 

require 'cute'
require 'logger'
require 'cute/taktuk'
require 'net/sftp'
require 'erb'
require 'socket'
require 'trollop'
require 'json'
require "net/http"
require "uri"
require "fileutils"


g5k = Cute::G5K::API.new()
user = g5k.g5k_user

# Get the script dir
scriptDir = File.expand_path(File.dirname(__FILE__))
# Make the temporary files directory (if not created already)
tempDir = scriptDir + "/.generated"
FileUtils.mkpath(tempDir)
FileUtils.mkpath(tempDir + "/config")

# Additionally create a directory for managed Ceph config files
prodDir = tempDir + "/config/prod"
FileUtils.mkpath(prodDir)

currentConfigFile = ""
if (["--def-conf", "-d"].include?(ARGV[0])  && !ARGV[1].empty? )
   currentConfigFile = ARGV[1] # assign config file location to variable configFile
   ARGV.delete_at(0)    # clean up ARGV array
   ARGV.delete_at(0)
else 
   currentConfigFile = tempDir + "/config/defaults.yml" # config file to be used.
   unless File.exist?(currentConfigFile)
     configFile = scriptDir + "/config/defaults.yml.example" # example config file
     FileUtils.cp(configFile, currentConfigFile)
   end # unless File.exist?
end    # if (["--def-conf", "-d"])

# Populate the hash with default parameters from YAML file.
defaults = begin
  YAML.load(File.open(currentConfigFile))
rescue ArgumentError => e
  puts "Could not parse YAML: #{e.message}"
end


# banner for script
opts = Trollop::options do
  version "ceph5k 0.0.6 (c) 2015-16 Anirvan BASU, INRIA RBA"
  banner <<-EOS
cephManaged is a script for creating RBD and FS on the Managed Ceph clusters in Grid'5000

Usage:
       cephManaged [options]
where [options] are:
EOS

  opt :ignore, "Ignore incorrect values"
  opt :jobid, "Oarsub ID of the client job", :default => 0
  opt :site, "Grid 5000 site for deploying Ceph cluster", :type => String, :default => defaults["site"]
  opt :cluster, "Grid 5000 cluster in specified site", :type => String, :default => defaults["cluster"]
  opt :walltime, "Wall time for Ceph cluster deployed", :type => String, :default => defaults["walltime"]

  opt :release, "Ceph Release name", :type => String, :default => defaults["release"]
  opt :'pool-name', "Pool name on Ceph cluster (userid_ added)", :type => String, :default => defaults["pool-name"]
  opt :'pool-size', "Pool size on Ceph cluster", :default => defaults["poolSize"]
  opt :'rbd-name', "RBD name for Ceph pool (userid_ added)", :type => String, :default => defaults["rbd-name"]
  opt :'rbd-size', "RBD size on Ceph pool", :default => defaults["rbd-size"]
  opt :'file', "File with clients list, same option as in kadeploy3", :type => String, :default => ""
  opt :'file-system', "File System to be formatted on created RBDs", :type => String, :default => defaults["file-system"]
  opt :'mnt-prod', "Mount point for RBD on managed cluster", :type => String, :default => defaults["mnt-prod"]

  opt :'job-client', "Grid'5000 job name for Ceph clients", :type => String, :default => defaults["job-client"]
  opt :'env-client', "G5K environment for client", :type => String, :default => defaults["env-client"]
  opt :'multi-client', "Multiple clients to access Ceph Managed cluster", :default => defaults["multi-client"]
  opt :'num-client', "Nodes in Ceph Client cluster", :default => defaults["num-client"]
  opt :'no-deployed', "Not using any deployed Ceph cluster", :default => defaults["no-deployed"]

end

# Move CLI arguments into variables. Later change to class attributes.
argJobID = opts[:jobid] # Oarsub ID of the client job. 
argSite = opts[:site] # site name. 
argG5KCluster = opts[:cluster] # G5K cluster name if specified. 
argWallTime = opts[:walltime] # walltime for the client reservation.

argRelease = opts[:release] # Ceph release name. 
argPoolName = "#{user}_" + opts[:'pool-name'] # Name of pool to create on clusters.
argPoolSize = opts[:'pool-size'] # Size of pool to create on clusters.
argRBDName = "#{user}_" + opts[:'rbd-name'] # Name of pool to create on clusters.
argRBDSize = opts[:'rbd-size'] # Size of pool to create on clusters.
argFileSystem = opts[:'file-system'] # File System to be formatted on created RBDs.
argMntProd = opts[:'mnt-prod'] # Mount point for RBD on production cluster.

argClientList = opts[:'file'] # File with clients list.
argOnlyDeploy = opts[:'only-deploy'] # Only deploy linux but don't configure Ceph client.
argEnvClient = opts[:'env-client'] # Grid'5000 environment to deploy Ceph clients. 
argJobClient = opts[:'job-client'] # Grid'5000 job name for Ceph clients. 
argMultiClient = opts[:'multi-client'] # Multiple clients to access Ceph Managed cluster.
argNumClient = opts[:'num-client'] # Nodes in Ceph Client cluster.
argNoDeployed = opts[:'no-deployed'] # Not using any deployed Ceph cluster.


# Next get job for Ceph clients
jobCephClient = nil # Ceph client job
clients = [] # Array of client nodes

# If client-list specified in CLI argument, get list of clients & fill variable
unless argClientList.empty?
   clients = File.open(argClientList, 'r'){ |file| 
      file.readlines.collect{ |line| line.chomp }
   }

   # Recover job details of client-list
   jobs = g5k.get_my_jobs(argSite, state = "running") 

   jobs.each do |job|
      if job["assigned_nodes"] == clients # get job where nodes are same as client-list
         jobCephClient = job
         puts "Ceph client job recovered with nodes: #{clients}"
       end # if job["assigned_nodes"] == clients
   end # jobs.each do |job|

else # when no client-list is specified. Do deployment or start from scratch.

   unless [nil, 0].include?(argJobID)
      # If jobID is specified, get the specific job
      jobCephClient = g5k.get_job(argSite, argJobID)
   else
      # Get all my jobs submitted in a site
      jobs = []
      ["waiting","running"].each do |state|
         jobs += g5k.get_my_jobs(argSite, state)

         # get the job with name "cephClient" or argJobName
         jobs.each do |job|
            if job["name"] == argJobClient # if job exists already, get nodes
               jobCephClient = job
            end # if job["name"] == argJobClient

         end # jobs.each do |job|

      end # ["waiting","running"].each do |state|

   end # if argJobID

   # If job state is "waiting" then wait for resources to be assigned
   unless jobCephClient.nil?

      if jobCephClient["state"] == "waiting"
         begin
            job = g5k.wait_for_job(jobCephClient, :wait_time => 60)
         rescue Cute::G5K::EventTimeout
            puts "Waited too long in site #{argSite}, releasing job #{argJobClient}"
            g5k.release(job)
         end
      end # if jobCephClient["state"] == "waiting"

   else
      # Finally, if job does not yet exist reserve nodes
      jobCephClient = g5k.reserve(:name => argJobClient, :nodes => argNumClient, :site => argSite, :cluster => argG5KCluster, :walltime => argWallTime, :type => :deploy) 

   end # unless jobCephClient.nil?

   # Assign roles to each node
   clients = jobCephClient["assigned_nodes"]

end # unless argClientList.empty?


# Additionally create a directory for saving details of clients deployed
jobID = jobCephClient["uid"]
clientStateDir = tempDir + "/#{argSite}/#{jobID}"
FileUtils.mkpath(clientStateDir)

# Prepare clients-list-file locally
clientsFile = File.open("#{clientStateDir}/clients-list", "w") do |file|
   clients.each do |client|
      file.puts("#{client}")
   end
end

# Abort script if only Linux deployment flag was set
abort("Linux #{argEnvClient} deployed on clients: #{clients}. \n Clients list file in: #{clientStateDir}/clients-list. \n Rerun script with option -f <nodes list file> to configure Ceph clients.") if argOnlyDeploy


# Get the client(s) for the managed Ceph cluster
# For a single client, this is the 'first' node of the job
clients = argMultiClient ? jobCephClient["assigned_nodes"] : [jobCephClient["assigned_nodes"][0]]

# if not yet deployed, then deploy it
if jobCephClient["deploy"].nil?
   puts "Deploying #{argEnvClient} on client node(s): #{clients}"
   depCephClient = g5k.deploy(jobCephClient, :nodes => clients, :env => argEnvClient) 
   g5k.wait_for_deploy(jobCephClient)
end # if jobCephClient["deploy"].nil?

# Remind where is the Ceph client
puts "Managed Ceph client(s) on: #{clients}"


# Prepare ceph.conf file for managed Ceph cluster
configFile = File.open(prodDir + "/ceph.conf", "w") do |file|
   file.puts("[global]")
   file.puts("  mon initial members = ceph0")
   file.puts("  mon host = 172.16.111.30")
end

# Copy ceph.conf file to client state directory
FileUtils.mkpath(clientStateDir + "/prod")
FileUtils.cp("#{prodDir}/ceph.conf", "#{clientStateDir}/prod/ceph.conf")
FileUtils.cp("/tmp/ceph.client.#{user}.keyring", "#{clientStateDir}/prod/ceph.client.#{user}.keyring")

# Then put ceph.conf file to the client
Cute::TakTuk.start(clients, :user => "root") do |tak|
     tak.exec!("rm -rf prod/")
     tak.exec!("mkdir prod/ && touch prod/ceph.conf")
     tak.put("#{prodDir}/ceph.conf", "/root/prod/ceph.conf")
     tak.put("/tmp/ceph.client.#{user}.keyring", "/etc/ceph/ceph.client.#{user}.keyring")
     tak.loop()
end

# Created & pushed config file for Managed Ceph cluster.
puts "Created & pushed config file for managed Ceph cluster to client(s)."


# Creating Ceph pools on managed cluster.
puts "Creating Ceph pool on managed cluster ..."
poolsList = []
userPool = ""
userPoolMatch = ""
userRBD = ""
prodCluster = false
abortFlag = false
# Check Ceph pools & RBD on Managed cluster, using first client.
client = clients[0]
Cute::TakTuk.start([client], :user => "root") do |tak|
     tak.exec!("modprobe rbd")
     # Create RBD on managed cluster
     result = tak.exec!("rados -c /root/prod/ceph.conf --id #{user} lspools")

     poolsList = result[client][:output].split("\n")

     poolCount = 0
     poolsList.each do |pool|  # logic: it will take the alphabetic-last pool from user
        userRBD = ""
        if pool.include? "#{user}"
           userPool = pool
           poolCount += 1
           userPoolMatch = pool if pool.include? "#{argPoolName}" # Perfect match of pool name
           # Check if RBD is already created, may contain data
           resultPool = tak.exec!("rbd -c /root/prod/ceph.conf --id #{user} --pool #{userPool} ls")

           unless resultPool[client][:output].nil? # means no rbd in userPool
              if resultPool[client][:output].include? "#{argRBDName}" 
                 userRBD = argRBDName # There is an rbd with name argRBDName already
              end # if resultPool[client][:output].include?
           end # unless resultPool[client][:output].nil?

        end # if pool.include? "#{user}"

     end # poolsList.each do

     unless userPool.empty?
        # If multiple pools from user, then confused, so exit.
        abort("Script exited - multiple Ceph pools for #{user}") if poolCount > 1 && userPoolMatch.empty?

     else   # There is no pool created on managed Ceph
        puts "Create at least one RBD pool from the Ceph managed frontend\n\n"
        puts "Use this link to create pool: https://api.grid5000.fr/sid/storage/ceph/ui/"
        puts "Then rerun this script.\n"
        abortFlag = true
        break
     end # userPool.empty?
     tak.loop()
end

# Abort script if no pool in managed Ceph
abort("Script exited - no pool for user in Managed Ceph clusters") if abortFlag

# Create Ceph pools & RBD
Cute::TakTuk.start(clients, :user => "root") do |tak|
     if userRBD.empty? # There was no rbd created for the user. So create it.
        result = tak.exec!("rbd -c /root/prod/ceph.conf --id #{user} --pool #{userPool} create #{argRBDName} --size #{argRBDSize} -k /etc/ceph/ceph.client.#{user}.keyring")
puts result
     end # if userRBD.empty?
     tak.loop()
end



# Created Pool & RBD for Ceph cluster.
unless userPool.empty?
     puts "Created Ceph pool on managed cluster as follows :"
     puts "On managed cluster:\n"
     puts "Pool name: #{userPool} , RBD Name: #{argRBDName} , RBD Size: #{argRBDSize} "
end # unless userPool.empty?



# Map RBD and create File Systems.
puts "Mapping RBD in managed Ceph clusters ..."
Cute::TakTuk.start(clients, :user => "root") do |tak|
     if argNoDeployed
        # Map RBD & create FS on deployed Ceph cluster
        result = tak.exec!("rbd map #{argRBDName} --pool #{argPoolName}")
puts result
        tak.exec!("mkfs.#{argFileSystem} -m0 /dev/rbd/#{argPoolName}/#{argRBDName}")
        if result[client][:status] == 0
           puts "Mapped RBD #{argRBDName} on deployed Ceph."
        end # if result[client][:status]
     end # if argNoDeployed

     myRBDName = ""
     # Map RBD & create FS on managed cluster
     result = tak.exec!("rbd -c /root/prod/ceph.conf --id #{user} --pool #{userPool} map #{argRBDName} -k /etc/ceph/ceph.client.#{user}.keyring")
     if userRBD.empty? # Do it only the first time when the RBD is created.
        tak.exec!("mkfs.#{argFileSystem} -m0 /dev/rbd/#{userPool}/#{argRBDName}")
        myRBDName = argRBDName
     else              # This case is when RBD is already created earlier.
        myRBDName = userRBD
     end # if userRBD.empty?
     client = result.keys
     if result[client][:status] == 0
        puts "Mapped RBD #{myRBDName} on managed Ceph."
     end

     tak.loop()
end


# Mount RBDs as File Systems.
puts "Mounting RBD as File Systems in managed Ceph clusters ..."
Cute::TakTuk.start(clients, :user => "root") do |tak|

     result = nil
     # mount RBD from managed cluster
     if userRBD.empty? # Do it only the first time when the RBD is created.
        tak.exec!("umount /dev/rbd/#{userPool}/#{argRBDName} /mnt/#{argMntProd}")
        tak.exec!("rmdir /mnt/#{argMntProd}")
        tak.exec!("mkdir /mnt/#{argMntProd}")
        result = tak.exec!("mount /dev/rbd/#{userPool}/#{argRBDName} /mnt/#{argMntProd}")
     else              # This case is when RBD is already created earlier.
        tak.exec!("umount /dev/rbd/#{userPool}/#{userRBD} /mnt/#{argMntProd}")
        tak.exec!("rmdir /mnt/#{argMntProd}")
        tak.exec!("mkdir /mnt/#{argMntProd}")
        result = tak.exec!("mount /dev/rbd/#{userPool}/#{userRBD} /mnt/#{argMntProd}")
     end # if userRBD.empty?
puts result.keys
     if result[client][:status] == 0
        puts "Mounted RBD as File System on managed Ceph."
     end

     tak.loop()
end

